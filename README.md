# Implementing-the-Transformer-architecture-from-scratch
It involves building its core components, primarily the encoder and decoder, which rely heavily on the self-attention mechanism.
